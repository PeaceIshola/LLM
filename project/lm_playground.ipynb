{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe523821",
      "metadata": {
        "id": "fe523821"
      },
      "source": [
        "# Build an LLM Playground"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e82492",
      "metadata": {
        "id": "08e82492"
      },
      "source": [
        "---\n",
        "## Learning Objectives  \n",
        "- Understand tokenization and how raw text is converted into a sequence of discrete tokens\n",
        "- Inspect GPT-2 and the Transformer architecture\n",
        "- Learn how to load pretrained LLMs using Hugging Face\n",
        "- Explore decoding strategies to generate text from LLMs\n",
        "- Compare completion models with instruction-tuned models\n",
        "\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1235110e",
      "metadata": {
        "id": "1235110e"
      },
      "outputs": [],
      "source": [
        "# Confirm required libraries are installed and working.\n",
        "import torch, transformers, tiktoken\n",
        "print(\"torch\", torch.__version__, \"| transformers\", transformers.__version__)\n",
        "print(\"âœ… Environment check complete. You're good to go!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c1eb0b",
      "metadata": {
        "id": "d4c1eb0b"
      },
      "source": [
        "# 1 - Tokenization\n",
        "\n",
        "A neural network cannot process raw text directly. It needs numbers.\n",
        "Tokenization is the process of converting text into numerical IDs that models can understand. In this section, you will learn how tokenization works in practice and why it is an essential step in every language model pipeline.\n",
        "\n",
        "Tokenization methods generally fall into three main categories:\n",
        "1. Word-level\n",
        "2. Character-level\n",
        "3. Subword-level\n",
        "\n",
        "### 1.1 - Word-level tokenization\n",
        "This method splits text by whitespace and treats each word as a single token. In the next cell, you will implement a basic word-level tokenizer by building a vocabulary that maps words to IDs and writing `encode` and `decode` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d784a288",
      "metadata": {
        "id": "d784a288"
      },
      "outputs": [],
      "source": [
        "# Creating a tiny corpus. In practice, a corpus is generally the entire internet-scale dataset used for training.\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Tokenization converts text to numbers\",\n",
        "    \"Large language models predict the next token\"\n",
        "]\n",
        "\n",
        "# Step 1: Build vocabulary (all unique words in the corpus) and mappings\n",
        "vocab = []\n",
        "word2id = {}\n",
        "id2word = {}\n",
        "\n",
        "for sentence in corpus:\n",
        "  words = sentence.split()\n",
        "  for word in words:\n",
        "    if word not in vocab:\n",
        "      vocab.append(word)\n",
        "\n",
        "for idx, word in enumerate(vocab):\n",
        "  word2id[word] = idx\n",
        "  id2word[idx] = word\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)} words\")\n",
        "print(\"First 15 vocab entries:\", vocab[:15])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b735a7b5",
      "metadata": {
        "id": "b735a7b5"
      },
      "outputs": [],
      "source": [
        "def encode(text):\n",
        "    # converts text to token IDs\n",
        "\n",
        "    words = text.split()\n",
        "    token_ids = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in word2id:\n",
        "            token_ids.append(word2id[word])\n",
        "        else:\n",
        "            token_ids.append(-1)\n",
        "    return token_ids\n",
        "\n",
        "\n",
        "def decode(ids):\n",
        "    # converts token IDs back to text\n",
        "\n",
        "    words = []\n",
        "\n",
        "    for idx in ids:\n",
        "        if idx in id2word:                   # if ID exists in dictionary\n",
        "            words.append(id2word[idx])       # append corresponding word\n",
        "        else:\n",
        "            words.append(\"<UNK>\")            # unknown ID â†’ placeholder token\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb614b9e",
      "metadata": {
        "id": "cb614b9e"
      },
      "outputs": [],
      "source": [
        "# Step 3: Test your tokenizer with random sentences.\n",
        "# Try a sentence with unseen words and see what happens (and how to fix it)\n",
        "\n",
        "test_sentence_1 = \"The quick brown fox\"\n",
        "encoded_1 = encode(test_sentence_1)\n",
        "decoded_1 = decode(encoded_1)\n",
        "\n",
        "print(encoded_1)\n",
        "print(decoded_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a379bac7",
      "metadata": {
        "id": "a379bac7"
      },
      "source": [
        "### 1.2 - Character-level tokenization\n",
        "\n",
        "In this approach, every single character (including spaces, punctuation, and even emojis) is assigned its own ID.\n",
        "\n",
        "In the next section, we will rebuild a tokenizer using the same corpus as before, but this time with a character-level approach.\n",
        "For simplicity, assume we are only using lowercase and uppercase English letters (a-z, A-Z)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac29144",
      "metadata": {
        "id": "4ac29144"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "# Step 1: Create a vocabulary that includes all uppercase and lowercase letters.\n",
        "vocab = []\n",
        "char2id = {}\n",
        "id2char = {}\n",
        "\n",
        "\n",
        "# 1. Add two special tokens for unknown characters and padding\n",
        "vocab.append(\"<PAD>\")     # Used for padding if needed\n",
        "vocab.append(\"<UNK>\")     # Represents any non-supported character\n",
        "\n",
        "# 2. Add lowercase aâ€“z\n",
        "for ch in string.ascii_lowercase:\n",
        "    vocab.append(ch)\n",
        "\n",
        "# 3. Add uppercase Aâ€“Z\n",
        "for ch in string.ascii_uppercase:\n",
        "    vocab.append(ch)\n",
        "\n",
        "# 4. Build both mapping dictionaries\n",
        "for idx, ch in enumerate(vocab):\n",
        "    char2id[ch] = idx\n",
        "    id2char[idx] = ch\n",
        "\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)} (52 letters + 2 specials)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4baab303",
      "metadata": {
        "id": "4baab303"
      },
      "outputs": [],
      "source": [
        "# Step 2: Implement encode() and decode() functions to convert between text and IDs.\n",
        "def encode(text):\n",
        "    # convert text to list of IDs\n",
        "    encoded_ids = []\n",
        "    for ch in text:\n",
        "        if ch in char2id:\n",
        "            encoded_ids.append(char2id[ch])\n",
        "        else:\n",
        "            encoded_ids.append(char2id[\"<UNK>\"])\n",
        "    return encoded_ids\n",
        "\n",
        "\n",
        "def decode(ids):\n",
        "    # Convert list of IDs back to text\n",
        "    decoded_chars = []\n",
        "    for idx in ids:\n",
        "        if idx in id2char:\n",
        "            decoded_chars.append(id2char[idx])\n",
        "        else:\n",
        "            decoded_chars.append(\"<UNK>\")\n",
        "    return \"\".join(decoded_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ede2771",
      "metadata": {
        "id": "6ede2771"
      },
      "outputs": [],
      "source": [
        "# Step 3: Test your tokenizer on a short sample word.\n",
        "sample_text = \"HelloWorld?!\"       # includes an unsupported \"!\" to test <UNK>\n",
        "encoded_output = encode(sample_text)\n",
        "decoded_output = decode(encoded_output)\n",
        "\n",
        "print(\"Original Text:\", sample_text)\n",
        "print(\"Encoded IDs  :\", encoded_output)\n",
        "print(\"Decoded Text :\", decoded_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391275bd",
      "metadata": {
        "id": "391275bd"
      },
      "source": [
        "### 1.3 - Subword-level tokenization\n",
        "\n",
        "Sub-word methods such as `Byte-Pair Encoding (BPE)`, `WordPiece`, and `SentencePiece` **learn** common groups of characters and merge them into tokens. For example, the word **unbelievable** might turn into three tokens: **[\"un\", \"believ\", \"able\"]**. This approach strikes a balance between word-level and character-level methods and fix their limitations.\n",
        "\n",
        "The BPE algorithm builds a vocabulary iteratively using the following process:\n",
        "1. Start with individual characters (each character is a token).\n",
        "2. Count all adjacent pairs of tokens in a large text corpus.\n",
        "3. Merge the most frequent pair into a new token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4675e67a",
      "metadata": {
        "id": "4675e67a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Step 1: Load a pretrained GPT-2 tokenizer from Hugging Face.\n",
        "# Refer to this to learn more: https://huggingface.co/docs/transformers/en/model_doc/gpt2\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c325134a",
      "metadata": {
        "id": "c325134a"
      },
      "outputs": [],
      "source": [
        "# Step 2: Use it to write encode and decode helper functions\n",
        "def encode(text):\n",
        "     return tokenizer.encode(text)\n",
        "\n",
        "\n",
        "def decode(ids):\n",
        "        return tokenizer.decode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55325562",
      "metadata": {
        "id": "55325562"
      },
      "outputs": [],
      "source": [
        "# 3. Inspect the tokens to see how BPE breaks words apart.\n",
        "sample = \"Unbelievable tokenization powers! ðŸš€\"\n",
        "\n",
        "# 3. Inspect the tokens to see how BPE breaks words apart.\n",
        "sample = \"Unbelievable tokenization powers! ðŸš€\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sample)   # view subword tokens\n",
        "token_ids = tokenizer.encode(sample)  # numeric IDs\n",
        "\n",
        "print(\"Original text:\", sample)\n",
        "print(\"Subword tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "badaa5a8",
      "metadata": {
        "id": "badaa5a8"
      },
      "source": [
        "### 1.4 - TikToken\n",
        "\n",
        "`tiktoken` is a fast, production-ready library for tokenization used by OpenAI models.\n",
        "It is designed for efficiency and consistency with how OpenAI counts tokens in GPT models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7704c470",
      "metadata": {
        "id": "7704c470"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Compare GPT-2 and GPT-4 tokenizers using tiktoken.\n",
        "\n",
        "# Step 1: Load two tokenizers\n",
        "gpt2_tok = tiktoken.get_encoding(\"gpt2\")\n",
        "gpt4_tok = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# Step 2: Encode the same sentence with both and observe how they differ\n",
        "sentence = \"The ðŸŒŸ star-programmer implemented AGI overnight and i love it.\"\n",
        "\n",
        "gpt2_ids = gpt2_tok.encode(sentence)\n",
        "gpt4_ids = gpt4_tok.encode(sentence)\n",
        "\n",
        "print(\"Original text:\", sentence)\n",
        "\n",
        "print(\"\\n=== GPT-2 TOKENIZER (gpt2) ===\")\n",
        "print(\"Token IDs:\", gpt2_ids)\n",
        "print(\"Number of tokens:\", len(gpt2_ids))\n",
        "\n",
        "print(\"\\n=== GPT-4 TOKENIZER (cl100k_base) ===\")\n",
        "print(\"Token IDs:\", gpt4_ids)\n",
        "print(\"Number of tokens:\", len(gpt4_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a758ba",
      "metadata": {
        "id": "c2a758ba"
      },
      "source": [
        "# 2. What is a Language Model?\n",
        "\n",
        "At its core, a **language model (LM)** is just a *very large* mathematical function built from many neural-network layers.  \n",
        "Given a sequence of tokens `[tâ‚, tâ‚‚, â€¦, tâ‚™]`, it learns to output a probability for the next token `tâ‚™â‚Šâ‚`.\n",
        "\n",
        "### 2.1 - A Single `Linear` Layer\n",
        "\n",
        "Before jumping into Transformers, let's start with the simplest building block: a `Linear` layer.\n",
        "\n",
        "A Linear layer computes `y = Wx + b`.\n",
        "\n",
        "Where:  \n",
        "  * `x` - input vector  \n",
        "  * `W` - weight matrix (learned)  \n",
        "  * `b` - bias vector (learned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e425948a",
      "metadata": {
        "id": "e425948a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a MyLinear PyTorch module and perform y = Wx + b.\n",
        "\n",
        "class MyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(MyLinear, self).__init__()\n",
        "        # Initialize weights and bias as learnable parameters.\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.randn(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Matrix multiplication followed by bias addition\n",
        "        return torch.matmul(self.weight, x) + self.bias    # y = Wx + b\n",
        "\n",
        "\n",
        "lin = MyLinear(3, 2)\n",
        "x = torch.tensor([1.0, -1.0, 0.5])\n",
        "print(\"Input :\", x)\n",
        "print(\"Weights:\", lin.weight)\n",
        "print(\"Bias   :\", lin.bias)\n",
        "print(\"Output :\", lin(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e5e225",
      "metadata": {
        "id": "13e5e225"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn, torch\n",
        "\n",
        "# Create a linear layer using pytorch's nn.Linear\n",
        "lin = nn.Linear(in_features=3, out_features=2)\n",
        "\n",
        "x = torch.tensor([1.0, -1.0, 0.5])\n",
        "print(\"Input :\", x)\n",
        "print(\"Weights:\", lin.weight)\n",
        "print(\"Bias   :\", lin.bias)\n",
        "print(\"Output :\", lin(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04f56bf",
      "metadata": {
        "id": "a04f56bf"
      },
      "source": [
        "### 2.2 - A `Transformer` Layer\n",
        "\n",
        "Most LLMs are a **stack of identical Transformer blocks**. Each block fuses two main components:\n",
        "\n",
        "| Step | What it does | Where it lives in code |\n",
        "|------|--------------|------------------------|\n",
        "| **Multi-Head Self-Attention** | Every token looks at every other token and decides *what matters*. | `block.attn` |\n",
        "| **Feed-Forward Network (MLP)** | Re-mixes information token-by-token. | `block.mlp` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47c87f6e",
      "metadata": {
        "id": "47c87f6e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Step 1: load the smallest GPT-2 model (124M parameters) using the Hugging Face transformers library.\n",
        "# Refer to: https://huggingface.co/docs/transformers/en/model_doc/gpt2\n",
        "# Step 1: load the smallest GPT-2 model (124M parameters)\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Step 2: inspect (print) the first Transformer block\n",
        "print(model.transformer.h[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84e277ea",
      "metadata": {
        "id": "84e277ea"
      },
      "source": [
        "In this section, you will run a minimal forward pass through one GPT-2 block to understand how tokens are transformed inside the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92df06df",
      "metadata": {
        "id": "92df06df"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create a small dummy input with a sequence of 8 random token IDs.\n",
        "batch_size = 2\n",
        "seq_len = 16\n",
        "dummy_ids = torch.randint(low=0, high=model.config.vocab_size, size=(batch_size, seq_len))\n",
        "\n",
        "# Step 2: Convert token IDs into embeddings (token + positional)\n",
        "token_embeds = model.transformer.wte(dummy_ids)                      # (B, T, hidden)\n",
        "pos_ids = torch.arange(seq_len).unsqueeze(0)                         # (1, T)\n",
        "pos_embeds = model.transformer.wpe(pos_ids)                          # (1, T, hidden)\n",
        "hidden_states = token_embeds + pos_embeds                            # (B, T, hidden)\n",
        "\n",
        "# Step 3: Pass the embeddings through a single Transformer block\n",
        "block_out = model.transformer.h[0](hidden_states)[0]                 # first block output\n",
        "\n",
        "# Step 4: Inspect the result\n",
        "print(\"Output shape:\", block_out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8493ecc8",
      "metadata": {
        "id": "8493ecc8"
      },
      "source": [
        "### 2.3 - Inside GPT-2\n",
        "\n",
        "GPT-2 is essentially a stack of identical Transformer blocks arranged in sequence.\n",
        "Each block contains attention, feed-forward, and normalization layers that process token representations step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78ddee1",
      "metadata": {
        "id": "a78ddee1"
      },
      "outputs": [],
      "source": [
        "# Print the name of all layers inside gpt.transformer.\n",
        "# You may find this helpful: https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_children\n",
        "\n",
        "# Print the name of all layers inside gpt.transformer.\n",
        "for name, layer in model.transformer.named_children():\n",
        "    print(name, \":\", type(layer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed029847",
      "metadata": {
        "id": "ed029847"
      },
      "source": [
        "As you can see, the Transformer holds various modules, arranged from a list of blocks (`h`). The following table summarizes these modules:\n",
        "\n",
        "| Step | What it does | Why it matters |\n",
        "|------|--------------|----------------|\n",
        "| **Token â†’ Embedding** | Converts IDs to vectors | Gives the model a numeric â€œhandleâ€ on words |\n",
        "| **Positional Encoding** | Adds â€œwhere am I?â€ info | Order matters in language |\n",
        "| **Multi-Head Self-Attention** | Each token asks â€œwhich other tokens should I look at?â€ | Lets the model relate words across a sentence |\n",
        "| **Feed-Forward Network** | Two stacked Linear layers with a non-linearity | Mixes information and adds depth |\n",
        "| **LayerNorm & Residual** | Stabilize training and help gradients flow | Keeps very deep networks trainable |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6a7495",
      "metadata": {
        "id": "0a6a7495"
      },
      "source": [
        "### 2.4 LLM's output\n",
        "\n",
        "When you pass a sequence of tokens through a language model, it produces a tensor of logits with shape\n",
        "`(batch_size, seq_len, vocab_size)`.\n",
        "Each position in the sequence receives a vector of scores representing how likely every possible token is to appear next. By applying a softmax function on the last dimension, these logits can be converted into probabilities that sum to 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98b7b34",
      "metadata": {
        "id": "f98b7b34"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "# Step 1: Load GPT-2 model and its tokenizer\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37894624",
      "metadata": {
        "id": "37894624"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tokenize input text\n",
        "text = \"Hello my name\"\n",
        "\n",
        "text = \"Hello my name\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")   # contains 'input_ids' and 'attention_mask'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf40003",
      "metadata": {
        "id": "ddf40003"
      },
      "outputs": [],
      "source": [
        "# Step 3: Pass the input IDs to the model\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)                   # forward pass\n",
        "logits = outputs.logits                         # shape: (batch, seq_len, vocab_size)\n",
        "\n",
        "print(\"Logits shape:\", logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "527da6c6",
      "metadata": {
        "id": "527da6c6"
      },
      "outputs": [],
      "source": [
        "# Step 4: Predict the next token\n",
        "# We take the logits from the final position, apply softmax to get probabilities,\n",
        "# and then extract the top 5 most likely next tokens. You may find F.softmax and torch.topk helpful in your implementation.\n",
        "\n",
        "last_token_logits = logits[0, -1, :]            # logits for final token position\n",
        "probs = F.softmax(last_token_logits, dim=-1)    # convert to probabilities\n",
        "top_probs, top_ids = torch.topk(probs, k=5)     # top 5 token IDs\n",
        "\n",
        "print(\"\\nTop 5 predicted next tokens:\")\n",
        "for prob, token_id in zip(top_probs, top_ids):\n",
        "    print(f\"{tokenizer.decode([token_id])!r}  ->  probability: {prob.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ccf391",
      "metadata": {
        "id": "e0ccf391"
      },
      "source": [
        "# 3 - Text Generation (Decoding)\n",
        "Once a language model has been trained to predict token probabilities, we can use it to generate text.\n",
        "This process is called text generation or decoding.\n",
        "\n",
        "At each step, the model outputs a probability distribution over possible next tokens.\n",
        "A decoding algorithm then selects one token based on that distribution, appends it to the sequence, and repeats the process to build text word by word. Different decoding strategies control how the model chooses the next token and how creative or deterministic the output will be. For example:\n",
        "- **Greedy** decoding: always pick the token with the highest probability. Simple and consistent, but often repetitive.\n",
        "- **Top-k** or **Nucleus** (top-p) sampling: randomly sample from the top few likely tokens to add variety.\n",
        "- Beam search: explores multiple candidate continuations and keeps the best overall sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac0c5728",
      "metadata": {
        "id": "ac0c5728"
      },
      "source": [
        "### 3.1 - Greedy decoding\n",
        "In this section, you will use GPT-2 and Hugging Face's built-in generate method to produce text using the greedy decoding strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2cb953",
      "metadata": {
        "id": "2f2cb953"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "model_id = \"gpt2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
        "\n",
        "\n",
        "# Step 1. Load GPT-2 model and tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "model.eval()\n",
        "\n",
        "# Step 2. Implement a text generation function using HuggingFace's generate method.\n",
        "def generate(model, tokenizer, prompt, max_new_tokens=128):\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)   # tokenize and move to device\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,      # Greedy decoding\n",
        "        )\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe777ba",
      "metadata": {
        "id": "dbe777ba"
      },
      "outputs": [],
      "source": [
        "tests=[\"Once upon a time\",\"What is 2+2?\", \"Suggest a party theme.\"]\n",
        "for prompt in tests:\n",
        "    print(f\"\\n GPT-2 | Greedy\")\n",
        "    print(generate(model, tokenizer, prompt, 80))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91607661",
      "metadata": {
        "id": "91607661"
      },
      "source": [
        "### 3.2 - Top-k and top-p sampling\n",
        "The generate function you implemented earlier can easily be extended to use different decoding strategies.\n",
        "\n",
        "In this section, you will reimplement the same function but adapt it to support Top-k and Top-p (nucleus) sampling. These methods introduce controlled randomness, allowing the model to explore multiple plausible continuations instead of always choosing the single most likely next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0633d4a3",
      "metadata": {
        "id": "0633d4a3"
      },
      "outputs": [],
      "source": [
        "# Implement `generate` to support 3 strategies: greedy, top_k, and top_o\n",
        "# You may find this link helpful: https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
        "\n",
        "def generate(model, tokenizer, prompt, strategy=\"greedy\", max_new_tokens=128):\n",
        "\n",
        "    # Tokenize prompt and move to model's device\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Configure decoding based on chosen strategy\n",
        "    if strategy == \"greedy\":\n",
        "        # Always select highest-probability token\n",
        "        gen_kwargs = dict(do_sample=False)\n",
        "\n",
        "    elif strategy == \"top_k\":\n",
        "        # Sample from top-k most likely tokens\n",
        "        gen_kwargs = dict(\n",
        "            do_sample=True,\n",
        "            top_k=50,            # You may tune this (10-100 typical)\n",
        "            top_p=1.0,           # Disabled for top-k\n",
        "            temperature=1.0\n",
        "        )\n",
        "\n",
        "    elif strategy == \"top_p\":\n",
        "        # Nucleus sampling: sample from tokens whose cumulative prob â‰¥ top_p\n",
        "        gen_kwargs = dict(\n",
        "            do_sample=True,\n",
        "            top_k=0,             # Disabled for top-p (or leave unrestricted)\n",
        "            top_p=0.9,           # You may tune this (0.8-0.95 typical)\n",
        "            temperature=1.0\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"strategy must be one of: 'greedy', 'top_k', or 'top_p'\")\n",
        "\n",
        "    # Generate output token IDs\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            **gen_kwargs\n",
        "        )\n",
        "\n",
        "    # Decode token IDs into string\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ca8865",
      "metadata": {
        "id": "24ca8865"
      },
      "outputs": [],
      "source": [
        "tests = [\"Once upon a time\", \"What is 2+2?\", \"Suggest a party theme.\"]\n",
        "\n",
        "for strategy in [\"greedy\", \"top_k\", \"top_p\"]:\n",
        "    print(f\"\\n===== Strategy: {strategy.upper()} =====\")\n",
        "    for prompt in tests:\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        print(generate(model, tokenizer, prompt, strategy=strategy, max_new_tokens=60))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b775b02",
      "metadata": {
        "id": "6b775b02"
      },
      "source": [
        "# 4 - Completion vs. Instruction-tuned LLMs\n",
        "\n",
        "So far, we have used `GPT-2` to generate text from a given input prompt. However, `GPT-2` is just a completion model. It simply continues the provided text without understanding it as a task or question. It is not designed to engage in dialogue or follow instructions.\n",
        "\n",
        "In contrast, instruction-tuned LLMs (such as `Qwen-Chat`) undergo an additional post-training stage after base pre-training. This process fine-tunes the model to behave helpfully and safely when interacting with users. Because of this extra stage, instruction-tuned models can:\n",
        "\n",
        "- Interpret prompts as requests rather than just text to continue\n",
        "- Stay in conversation mode, answering questions and following steps\n",
        "- Handle refusals and safety boundaries appropriately\n",
        "- Maintain a consistent helpful persona, rather than drifting into storytelling\n",
        "\n",
        "### 4.1 - `Qwen/Qwen3-0.6B` vs. `GPT2`\n",
        "\n",
        "In the next cell, you will feed the same prompt to two different models:\n",
        "\n",
        "- GPT-2 (completion-only): continues the text in the same writing style\n",
        "- Qwen/Qwen3-0.6B (instruction-tuned): interprets the input as an instruction and responds helpfully\n",
        "\n",
        "Comparing the two outputs will make the difference between completion and instruction-tuned behavior clear.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b73e7a",
      "metadata": {
        "id": "57b73e7a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load both GPT-2 and Qwen models using HuggingFace `.from_pretrained` method.\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----- Load GPT-2 (Completion Model) -----\n",
        "gpt2_name = \"gpt2\"\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_name)\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_name).to(device)\n",
        "gpt2_model.eval()\n",
        "\n",
        "# ----- Load Qwen (Instruction-Tuned Model) -----\n",
        "# using a small but instruction-trained chat model\n",
        "qwen_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_name)\n",
        "qwen_model = AutoModelForCausalLM.from_pretrained(qwen_name).to(device)\n",
        "qwen_model.eval()\n",
        "\n",
        "print(\"Models loaded successfully:\")\n",
        "print(f\"- Completion model     : {gpt2_name}\")\n",
        "print(f\"- Instruction-tuned LLM: {qwen_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef49ab1b",
      "metadata": {
        "id": "ef49ab1b"
      },
      "source": [
        "We have now downloaded two small checkpoints: GPT-2 (124M parameters) and Qwen3-0.6B (600M parameters). If the previous cell took some time to run, that was mainly due to model download speed. The models will be cached locally, so future runs will be faster.\n",
        "\n",
        "Next, we will generate text using our generate function with both models and the same prompt to directly compare how a completion-only model (GPT-2) behaves differently from an instruction-tuned model (Qwen)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c78a508",
      "metadata": {
        "id": "0c78a508"
      },
      "outputs": [],
      "source": [
        "tests = [(\"Once upon a time\", \"greedy\"),\n",
        "         (\"What is 2+2?\", \"top_k\"),\n",
        "         (\"Suggest a party theme.\", \"top_p\")]\n",
        "\n",
        "for prompt, strategy in tests:\n",
        "    print(f\"\\n=== Prompt: {prompt!r} | Strategy: {strategy} ===\")\n",
        "\n",
        "    # GPT-2 output\n",
        "    gpt2_out = generate(gpt2_model, gpt2_tokenizer, prompt, strategy=strategy, max_new_tokens=80)\n",
        "    print(\"\\n[GPT-2 Completion Model]\")\n",
        "    print(gpt2_out)\n",
        "\n",
        "    # Qwen output\n",
        "    qwen_out = generate(qwen_model, qwen_tokenizer, prompt, strategy=strategy, max_new_tokens=80)\n",
        "    print(\"\\n[Qwen Instruction-Tuned Model]\")\n",
        "    print(qwen_out)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_playground",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
