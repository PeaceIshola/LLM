# LLM Projects

This repository contains hands-on projects to learn about Large Language Models (LLMs) from foundational concepts to practical implementations.

## Project Structure

### Project 1: LLM Playground ✅ DONE
**Location:** `project/lm_playground.ipynb`

**What's Done:**
- ✅ Tokenization fundamentals (word-level, character-level, subword-level)
- ✅ Understanding BPE and TikToken
- ✅ Exploring Transformer architecture and GPT-2 internals
- ✅ Text generation with different decoding strategies (greedy, top-k, top-p)
- ✅ Comparing completion models vs. instruction-tuned models

**What You'll Learn:**
- How text is converted into tokens that LLMs can process
- The architecture of Transformer-based models
- How to load and use pretrained models from Hugging Face
- Different text generation strategies and their trade-offs
- Key differences between base completion models and instruction-tuned models

---

## Getting Started

Each project is self-contained in its own directory with a Jupyter notebook. You can run the notebooks:
1. **In Google Colab** - Click the Colab badge in each notebook
2. **Locally** - Use the provided `env.yaml` to set up a conda environment

## Prerequisites
- Basic Python knowledge
- Understanding of neural networks (helpful but not required)
- Curiosity about how LLMs work!
